{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.drop('seriousdlqin2yrs', axis=1)\n",
    "y = training_data.seriousdlqin2yrs\n",
    "features_label = training_data.columns[1:]\n",
    "forest = RandomForestClassifier (n_estimators = 10000, random_state=0, n_jobs = -1)\n",
    "forest.fit(X,y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np. argsort(importances)[::-1]\n",
    "for i in range(X.shape[1]):\n",
    "    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i],importances[indices[i]]))\n",
    "    \n",
    "    \n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),features_label, rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K 최근접 이웃**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knMod = KNeighborsClassifierClaasifier(n_neighbors=5,      #예측이 가장 가까은 다섯 개의 이웃을 기반으로 한다는 점을 의미\n",
    "                                       weights='uniform',  #각 이웃의 모든 점에 가중치가 일률적으로 적용\n",
    "                                       algorithm='auto',   #전달된 값을 기반으로 가장 적합한 알고리즘을 결정하려고 시도\n",
    "                                       leaf_size=30,       #모델 및 쿼리의 생성속도에 영향\n",
    "                                       p=2,                #p=2이면 유클리드 거리를 사용하겠다는 의미\n",
    "                                       metric='minkowski', \n",
    "                                       metric_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LogisticRegression\n",
    "glmMod = LogisticRegression(penalty='l1',        #경사 하강 알고리즘을 선택한 것을 나타냄 여기서는 뉴턴-켤레 경사도\n",
    "                            dual=False,          #표본개수 > 특징개수 라면 이 파라미터를 false로 설정 \n",
    "                            tol=0.0001,          #알고리즘의 중단 기준 중 하나\n",
    "                            c=1.0,               #regularization rkdehdml durdmf skxksoa\n",
    "                            fit_intercept=True,  #편향을 나타내는데 사용\n",
    "                            intercept_scaling=1, #\n",
    "                            class_weight=None,   #클래스 레이블과 관련된 가중치는 없다는 뜻\n",
    "                            random_state=None,   \n",
    "                            solver='liblinear', \n",
    "                            max_iter=100, \n",
    "                            multi_class='ovr',   #이진분류 문제임을 나타냄\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaMod = AdaBoostClassifier(base_estimator=None, \n",
    "                            n_estimators=200, \n",
    "                            learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbMod = GradientBoostingClassifier(loss='deviance',               #확률적 출력으로 분류하기 위해 로지스틱 회귀 분석을 사용한다는 점을 의미\n",
    "                                   learning_rate=0.1,        \n",
    "                                   n_estimators=200,              #수행해야 하는 증폭 단계 수 의미\n",
    "                                   subsample=1.0,                 #편향 및 분산 값을 조정하는데 도움 (<1.0 을 선택하면 분산이 줄어들고 편향이 증가)\n",
    "                                   min_samples_split=2,  \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_depth=3,                   #개별 회귀 추정기의 최대 깊이를 나타냄\n",
    "                                   init=None, \n",
    "                                   random_state=None, \n",
    "                                   max_features=None,             #N개의 특징이 있을을 나타냄\n",
    "                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfMod = RandomForestClassifier(n_estimators=10,               #포레스트의 트리 수\n",
    "                               criterion='gini',              #얻은 정보는 'gini'에 의해 계산됨\n",
    "                               max_depth=None,                #모든 leaf가 순수하거나 모든 leaf가 min_samples_split 표본보다 작아질 때까지 노드가 확장\n",
    "                               min_samples_split=2,           #트리를 생성하기 위해 분할을 수행하는데 필요한 최소 2개의 표본이 있음을 나타냄\n",
    "                               min_samples_leaf=1,            #리프 노드의 표본 크기\n",
    "                               min_weight_fraction_leaf=0.0,  #리프 노드에 있어야 하는 총 가증치 중 최소 가중치\n",
    "                               max_features='auto',           \n",
    "                               max_leaf_nodes=None,           #리프 노드가 무제한일 수 있음\n",
    "                               bootstrap=True,                \n",
    "                               oob_score=False,               #주머니 밖의 외부 표본을 사용할지 여부\n",
    "                               n_jobs=1,                      \n",
    "                               random_state=None, \n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
